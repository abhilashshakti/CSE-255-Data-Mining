{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import urllib\n",
    "import math\n",
    "%pylab inline\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStatistics.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.numpy_pack import packArray,unpackArray\n",
    "from spark_PCA import computeCov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from computeStatistics import computeOverAllDist, STAT_Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read weather data for state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/WA.tgz > ../Data/Weather/WA.tgz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 61.4M  100 61.4M    0     0  11.7M      0  0:00:05  0:00:05 --:--:-- 12.5M 0:00:02 10.1M\n",
      "-rw-r--r-- 1 jovyan users 62M May  6 09:18 ../Data/Weather/WA.tgz\n"
     ]
    }
   ],
   "source": [
    "state='WA'\n",
    "data_dir='../Data/Weather'\n",
    "\n",
    "tarname=state+'.tgz'\n",
    "parquet=state+'.parquet'\n",
    "\n",
    "!rm -rf $data_dir/$tarname\n",
    "\n",
    "command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s > %s/%s\"%(tarname,data_dir,tarname)\n",
    "print(command)\n",
    "!$command\n",
    "!ls -lh $data_dir/$tarname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/Sections/Data/Weather\n",
      "76112\t./WA.parquet\n",
      "/home/jovyan/work/Sections/Section2-Weather-PCA\n",
      "177336\n",
      "+-----------+-----------+----+--------------------+-----------------+-----------------+-------------------+-----------------+-----+----------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|         latitude|          longitude|        elevation|state|      name|\n",
      "+-----------+-----------+----+--------------------+-----------------+-----------------+-------------------+-----------------+-----+----------+\n",
      "|USC00459342|       PRCP|1901|[00 7E 00 7E 00 7...|126.7229995727539|45.79999923706055|-121.93329620361328|351.1000061035156|   WA|WIND RIVER|\n",
      "|USC00459342|       PRCP|1906|[00 7E 00 7E 00 7...|126.7229995727539|45.79999923706055|-121.93329620361328|351.1000061035156|   WA|WIND RIVER|\n",
      "|USC00459342|       PRCP|1907|[00 7E 00 7E 00 7...|126.7229995727539|45.79999923706055|-121.93329620361328|351.1000061035156|   WA|WIND RIVER|\n",
      "|USC00459342|       PRCP|1931|[00 7E 28 5B C0 5...|126.7229995727539|45.79999923706055|-121.93329620361328|351.1000061035156|   WA|WIND RIVER|\n",
      "|USC00459342|       PRCP|1932|[00 00 00 00 00 5...|126.7229995727539|45.79999923706055|-121.93329620361328|351.1000061035156|   WA|WIND RIVER|\n",
      "+-----------+-----------+----+--------------------+-----------------+-----------------+-------------------+-----------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cur_dir,=!pwd\n",
    "%cd $data_dir\n",
    "!tar -xzf $tarname\n",
    "!du ./$parquet\n",
    "%cd $cur_dir\n",
    "\n",
    "\n",
    "weather_df=sqlContext.read.parquet(data_dir+'/'+parquet)\n",
    "print(weather_df.count())\n",
    "weather_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read statistics information for state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: aws: not found\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls mas-dse-open/Weather/by_state/ | grep STAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read statistics\n",
    "filename='STAT_%s.pickle'%state\n",
    "command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s.gz > %s/%s.gz\"%(filename,data_dir,filename)\n",
    "print(command)\n",
    "!$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzpath='%s/%s.gz'%(data_dir,filename)\n",
    "print(gzpath)\n",
    "!ls -l $gzpath\n",
    "!gunzip -f $gzpath\n",
    "STAT,STAT_Descriptions = load(open(data_dir+'/'+filename,'rb'))\n",
    "print('keys from STAT=',STAT.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read information about US weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename='US_stations.tsv.gz'\n",
    "command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/Info/%s > %s/%s\"%(filename,data_dir,filename)\n",
    "print(command)\n",
    "!$command\n",
    "filename_no_gz = filename[:-3]\n",
    "!gunzip -f $data_dir/$filename\n",
    "!ls -lh $data_dir/US_stations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read csv into pandas dataframe\n",
    "PATH='%s/%s'%(data_dir,filename_no_gz)\n",
    "print(PATH)\n",
    "stations=pd.read_csv(PATH,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType, FloatType,BinaryType\n",
    "\n",
    "schema = StructType([StructField(\"Station\", StringType(), False),\n",
    "                     StructField(\"Dist_coast\", FloatType(), False),\n",
    "                     StructField(\"Latitude\", FloatType(), False),\n",
    "                     StructField(\"Longitude\", FloatType(), False),\n",
    "                     StructField(\"Elevation\", FloatType(), False),\n",
    "                     StructField(\"State\", StringType(), True),                  \n",
    "                     StructField(\"Name\", StringType(), False)])                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = sqlContext.createDataFrame(stations,schema).drop('State')\n",
    "stations_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jdf=weather_df.join(stations_df,on='Station',how='left')\n",
    "jdf.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jdf.select(['Station','Measurement','Year','Name']).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(jdf,'jdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get all measurements for a particular year and a particular station\n",
    "measurement='PRCP'\n",
    "Query=\"\"\"\n",
    "SELECT *\n",
    "FROM jdf \n",
    "WHERE Measurement='%s'\n",
    "AND Name='BUFFALO'\n",
    "ORDER BY YEAR\"\"\"%(measurement)\n",
    "df=sqlContext.sql(Query)\n",
    "print(df.count())\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(['Station','Year','Values']).schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Smoothing by convolving with gaussian window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from astropy.convolution import convolve\n",
    "from scipy import signal\n",
    "#using astrophy.convolution.convolve and not scipy.signal.convolve because the first can handle nans.\n",
    "\n",
    "orig_pdf=df.toPandas()\n",
    "orig_pdf.head()\n",
    "\n",
    "def Smoother(orig_pdf,order=101,std=20):\n",
    "    window = signal.gaussian(order, std=std)\n",
    "    window/=sum(window)\n",
    "\n",
    "    L=list(orig_pdf['Values'])\n",
    "\n",
    "    orig=np.stack([unpackArray(V,np.float16) for V in L])\n",
    "    orig_shape=orig.shape\n",
    "    orig=orig.flatten()\n",
    "\n",
    "    smoothed = convolve(orig, window)\n",
    "    smoothed=np.reshape(smoothed,orig_shape)\n",
    "\n",
    "    #create a new pandas dataframe\n",
    "    smoothed_pdf=orig_pdf.copy()   # make a copy\n",
    "\n",
    "    L=[packArray(smoothed[i,:]) for i in range(smoothed.shape[0])]\n",
    "    smoothed_pdf['Values']=L\n",
    "\n",
    "    smoothed_pdf.loc[0,'Measurement']\n",
    "\n",
    "    new_name = 'smooth_'+smoothed_pdf.loc[0,'Measurement']\n",
    "    smoothed_pdf['Measurement']=new_name\n",
    "    return smoothed_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_pdf=Smoother(orig_pdf)\n",
    "smoothed_pdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query=\"\"\"\n",
    "SELECT Station,count(Year) as count\n",
    "FROM jdf \n",
    "WHERE Measurement='%s'\n",
    "GROUP BY Station\n",
    "ORDER BY count\n",
    "\"\"\"%(measurement)\n",
    "stat_pdf=sqlContext.sql(Query).toPandas()\n",
    "stat_pdf.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations=list(stat_pdf['Station'])\n",
    "stations=stations[-1:0:-1]\n",
    "stations[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sqlf\n",
    "import pyspark\n",
    "import pyarrow\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ndf=Smoother(orig_pdf)\n",
    "ndf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "# get all measurements for a particular year and a particular station\n",
    "measurement='PRCP'\n",
    "Query_template=\"\"\"\n",
    "SELECT *\n",
    "FROM jdf \n",
    "WHERE Measurement='%s'\n",
    "AND Station='%s'\n",
    "ORDER BY YEAR\"\"\"\n",
    "\n",
    "for station in stations:\n",
    "    t0=time()\n",
    "    Query=Query_template%(measurement,station)\n",
    "\n",
    "    pdf=sqlContext.sql(Query).toPandas()\n",
    "    t1=time()\n",
    "    smoothed_pdf=Smoother(pdf)\n",
    "    t2=time()\n",
    "    smoothed_df= sqlContext.createDataFrame(smoothed_pdf)\n",
    "    jdf=jdf.union(smoothed_df)\n",
    "    t3=time()\n",
    "    print('Station=%s, rows=%d, prep=%5.2f,compute=%5.2f,cleanup=%f5.2,total=%f5.2'\n",
    "          %(station,pdf.shape[0],t1-t0,t2-t1,t3-t2,t3-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../Data/Weather/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfilename='../../Data/Weather/Joined_smoothed_PRCP.parquet'\n",
    "jdf.write.save(outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh $outfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf.columns)\n",
    "sdf = sqlContext.createDataFrame(pdf)\n",
    "sdf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'smoothed_%s'%(station),\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdf=jdf.union(sdf)\n",
    "jdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BinaryType not supported  by pandas_udf\n",
    "Running the following code: \n",
    "```python\n",
    "import pyspark.sql.functions as sqlf\n",
    "import pyspark\n",
    "import pyarrow\n",
    "pyspark.__version__  (2.3.0)\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "def Smoother(orig_pdf):\n",
    "    return orig_pdf\n",
    "\n",
    "### Offending command\n",
    "smoother_udf=pandas_udf(Smoother,df.select(['Station','Year','Values']).schema, PandasUDFType.GROUPED_MAP) \n",
    "\n",
    "X=df.groupby(\"Station\").apply(smoother_udf)\n",
    "X.show()\n",
    "```\n",
    "Generates the following error message\n",
    "```\n",
    "NotImplementedError: Invalid returnType with grouped map Pandas UDFs: StructType(List(StructField(Station,StringType,true),StructField(Year,IntegerType,true),StructField(Values,BinaryType,true))) is not supported\n",
    "```\n",
    "\n",
    "Works find if only ('Station','Year') are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lib.YearPlotter import YearPlotter\n",
    "fig, ax = plt.subplots(figsize=(10,7));\n",
    "YP=YearPlotter()\n",
    "YP.plot(smoothed[110:120,:].transpose(),fig,ax,title='smoothed %s for %s'%(measurement,stat));\n",
    "plt.savefig('percipitation.png')\n",
    "#title('A sample of graphs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7));\n",
    "YP=YearPlotter()\n",
    "i=85\n",
    "factor=5\n",
    "pair=np.stack([orig[i,:],smoothed[i,:]*factor])\n",
    "pair.shape\n",
    "\n",
    "YP.plot(pair.transpose(),fig,ax,title='smoothed %s for %s'%(measurement,stat));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from astropy.convolution import convolve\n",
    "window = signal.gaussian(81, std=20)\n",
    "\n",
    "window/=sum(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P=T[3,:]\n",
    "P[10:30]=np.nan\n",
    "f=filtered = convolve(P, window)\n",
    "print(len(f))\n",
    "plot(f)\n",
    "plot(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distribution of missing observations\n",
    "The distribution of missing observations is not uniform throughout the year. We visualize it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from MultiPlot import *                \n",
    "def plot_valid(m,fig,axis):\n",
    "    valid_m=STAT[m]['NE']\n",
    "    YP.plot(valid_m,fig,axis,title='valid-counts '+m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TMIN','TMAX'],plot_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TOBS','PRCP'],plot_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['SNOW', 'SNWD'],plot_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plots of mean and std of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mean_std(m,fig,axis):\n",
    "    scale=1.\n",
    "    temps=['TMIN','TMAX','TOBS']\n",
    "    percipitation=['PRCP','SNOW','SNWD']\n",
    "    _labels=['mean+std','mean','mean-std']\n",
    "    if (m in temps or m=='PRCP'):\n",
    "        scale=10.\n",
    "    mean=STAT[m]['Mean']/scale\n",
    "    std=np.sqrt(STAT[m]['Var'])/scale\n",
    "    graphs=np.vstack([mean+std,mean,mean-std]).transpose()\n",
    "    YP.plot(graphs,fig,axis,labels=_labels,title='Mean+-std   '+m)\n",
    "    if (m in temps):\n",
    "        axis.set_ylabel('Degrees Celsius')\n",
    "    if (m in percipitation):\n",
    "        axis.set_ylabel('millimeter')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TMIN','TMAX'],plot_mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TOBS','PRCP'],plot_mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single('TOBS',plot_mean_std,'r_figures/TOBS.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['SNOW', 'SNWD'],plot_mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_single('SNOW',plot_mean_std,'r_figures/SNOW.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single('SNWD',plot_mean_std,'r_figures/SNWD.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### plotting top 3 eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_eigen(m,fig,axis):\n",
    "    EV=STAT[m]['eigvec']\n",
    "    YP.plot(EV[:,:3],fig,axis,title='Top Eigenvectors '+m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TMIN','TMAX'],plot_eigen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TOBS','PRCP'],plot_eigen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['SNOW', 'SNWD'],plot_eigen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Script for plotting percentage of variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def pltVarExplained(j):\n",
    "    subplot(1,3,j)\n",
    "    EV=STAT[m]['eigval']\n",
    "    k=5\n",
    "    L=([0,]+list(cumsum(EV[:k])))/sum(EV)\n",
    "    #print m,L\n",
    "    plot(L)\n",
    "    title('Percentage of Variance Explained for '+ m)\n",
    "    ylabel('Percentage of Variance')\n",
    "    xlabel('# Eigenvector')\n",
    "    grid()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f=plt.figure(figsize=(15,4))\n",
    "j=1\n",
    "for m in ['TMIN', 'TOBS', 'TMAX']: #,\n",
    "    pltVarExplained(j)\n",
    "    j+=1\n",
    "f.savefig('r_figures/VarExplained1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f=plt.figure(figsize=(15,4))\n",
    "j=1\n",
    "for m in ['SNOW', 'SNWD', 'PRCP']:\n",
    "    pltVarExplained(j)\n",
    "    j+=1 \n",
    "f.savefig('r_figures/VarExplained2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "190px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "514px",
    "left": "0px",
    "right": "925px",
    "top": "107px",
    "width": "323px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
